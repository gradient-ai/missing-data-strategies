{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup \n",
    "install packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install miceforest catboost wget\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup \n",
    "\n",
    " Load in the dataset, clean it, and split it into test and training sets. \n",
    "\n",
    " We then display the Standard deviation, mean and median"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset\n",
    "# remove unwanted columns\n",
    "data = pd.read_csv('energy_efficiency_data.csv')\n",
    "data = data.drop('Cooling_Load', axis=1)\n",
    "\n",
    "# split the data into train and test sets\n",
    "train_df, test_df = train_test_split(data,test_size=0.25,random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display the standard deviation, mean and median\n",
    "original_desc = train_df.describe().T[['std', 'mean', '50%' ]]\n",
    "original_desc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load in and instantiate the CatBoostRegressor model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required packages\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# get the features and targets\n",
    "# for train and test sets\n",
    "x_train = train_df.drop('Heating_Load', axis=1)\n",
    "y_train = train_df['Heating_Load']\n",
    "x_test = test_df.drop('Heating_Load', axis=1)\n",
    "y_test = test_df['Heating_Load']\n",
    "\n",
    "# Build and train the model \n",
    "model = CatBoostRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "original_score = mean_absolute_error(y_test, model.predict(x_test))\n",
    "print(f'The MAE of the model using the original data is {original_score:.2f}') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simulating Missing At Random dataset\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required packages\n",
    "import torch\n",
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/BorisMuzellec/MissingDataOT/master/utils.py')\n",
    "from utils import *\n",
    "\n",
    "# Function produce_NA for generating missing values\n",
    "\n",
    "def produce_NA(X, p_miss, mecha=\"MAR\", opt=None, p_obs=None, q=None):\n",
    "    \n",
    "    to_torch = torch.is_tensor(X)\n",
    "    \n",
    "    if not to_torch:\n",
    "        X = X.astype(np.float32)\n",
    "        X = torch.from_numpy(X)\n",
    "    \n",
    "    if mecha == \"MAR\":\n",
    "        mask = MAR_mask(X, p_miss, p_obs).double()\n",
    "    elif mecha == \"MNAR\" and opt == \"logistic\":\n",
    "        mask = MNAR_mask_logistic(X, p_miss, p_obs).double()\n",
    "    elif mecha == \"MNAR\" and opt == \"quantile\":\n",
    "        mask = MNAR_mask_quantiles(X, p_miss, q, 1-p_obs).double()\n",
    "    elif mecha == \"MNAR\" and opt == \"selfmasked\":\n",
    "        mask = MNAR_self_mask_logistic(X, p_miss).double()\n",
    "    else:\n",
    "        mask = (torch.rand(X.shape) < p_miss).double()\n",
    "    \n",
    "    X_nas = X.clone()\n",
    "    X_nas[mask.bool()] = np.nan\n",
    "    \n",
    "    return X_nas.double()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get sample MAR data \n",
    "data_MAR = produce_NA(train_df.to_numpy(), p_miss=0.20, p_obs=0.75)\n",
    "\n",
    "# load MAR data into dataframe\n",
    "MAR_df = pd.DataFrame(data_MAR.numpy(), columns= train_df.columns)\n",
    "MAR_df.sample(5)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Total Percentage of missing values\n",
    "total_miss_percent = (sum(MAR_df.isnull().sum()) / (MAR_df.shape[0] *                                                             MAR_df.shape[1])) * 100\n",
    "print(f'Percentage of total missing data: {total_miss_percent:.0f}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "percent_missing = MAR_df.isnull().sum() * 100 / len(MAR_df)\n",
    "pd.DataFrame(percent_missing, columns = ['Percent_Missing'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Percent_miss_samples = ((len(MAR_df) - len(MAR_df.dropna())) / len(MAR_df)) *  100 \n",
    "print(f'Percentage of samples with missing data: {Percent_miss_samples:.0f}%')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Applying techniques for handling missing data\n",
    "\n",
    "## Listwise deletion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop sample with missing values\n",
    "listwise_df = MAR_df.dropna()\n",
    "\n",
    "# get data description after deletion\n",
    "listwise_desc = listwise_df.describe().T[['std', 'mean', '50%' ]]\n",
    "\n",
    "print(f'Number of samples before deletion: {len(MAR_df)}')\n",
    "print(f'Number of samples after deletion: {len(listwise_df)}')\n",
    "print(f'Number of samples after lost to listwise deletion: {len(MAR_df) - len(listwise_df)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## how does this affect performance?\n",
    "\n",
    "# Get the features and target variables\n",
    "# of the dataset after listwise deletion\n",
    "x_train = listwise_df.drop('Heating_Load', axis=1)\n",
    "y_train = listwise_df['Heating_Load']\n",
    "\n",
    "# building and evaluating the model\n",
    "model = CatBoostRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "listwise_score = mean_absolute_error(y_test, model.predict(x_test))\n",
    "print(f'The MAE of the model using listwise deletion is {listwise_score :.2f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imputation\n",
    "\n",
    "## simple imputation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required package\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# define mean imputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# get mean of variables and substitute missing values\n",
    "mean_imp_data = imputer.fit_transform(MAR_df.values)\n",
    "\n",
    "# get dataframe with mean imputed values\n",
    "mean_imp_df = pd.DataFrame(mean_imp_data, columns= MAR_df.columns)\n",
    "\n",
    "# get data description after mean imputation\n",
    "mean_imp_desc = mean_imp_df.describe().T[['std', 'mean', '50%' ]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the train features and target variables\n",
    "# of the mean imputed dataset\n",
    "x_train = mean_imp_df.drop('Heating_Load', axis=1)\n",
    "y_train = mean_imp_df['Heating_Load']\n",
    "\n",
    "# building and evaluating the model\n",
    "model = CatBoostRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "mean_imp_score = mean_absolute_error(y_test, model.predict(x_test))\n",
    "print(f'The MAE of the model using mean imputation is {mean_imp_score :.2f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model-Based Imputation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required packages\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# define model for imputation\n",
    "impute_estimator = KNeighborsRegressor()\n",
    "imputer = IterativeImputer(estimator=impute_estimator, max_iter=25, tol= 1e-1, random_state=0)\n",
    "\n",
    "# impute missing values using model imputer\n",
    "model_imp_data = np.round(imputer.fit_transform(MAR_df), 6)\n",
    "\n",
    "# get datafraame with model imputation\n",
    "model_imp_df = pd.DataFrame(model_imp_data, columns= MAR_df.columns)\n",
    "\n",
    "# get description of model imputed data\n",
    "model_imp_desc = model_imp_df.describe().T[['std', 'mean', '50%' ]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the train features and target variables\n",
    "# of the model imputed dataset\n",
    "x_train = model_imp_df.drop('Heating_Load', axis=1)\n",
    "y_train = model_imp_df['Heating_Load']\n",
    "\n",
    "# building and evaluating the model\n",
    "model = CatBoostRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "model_imp_score = mean_absolute_error(y_test,model.predict(x_test))\n",
    "print(f'The MAE of the model using model based imputation is {model_imp_score :.2f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Imputation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import required package\n",
    "import miceforest as mf\n",
    "\n",
    "# create kernel for MI\n",
    "kernel = mf.ImputationKernel(MAR_df, datasets=20, random_state= 0)\n",
    "\n",
    "# Run the MICE algorithm for 5 iterations\n",
    "#  on each of the datasets\n",
    "kernel.mice(5)\n",
    "\n",
    "# show the number of datasets\n",
    "print(f'Number of datasets with imputations: {kernel.dataset_count()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For each imputed dataset, train a catbost regressor\n",
    "predictions = []\n",
    "for i in range(kernel.dataset_count()):\n",
    "    MICE_imp_df = kernel.complete_data(i)\n",
    "    x_train = MICE_imp_df.drop('Heating_Load', axis=1)\n",
    "    y_train = MICE_imp_df['Heating_Load']\n",
    "    model = CatBoostRegressor()\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions.append(model.predict(x_test)) # add test prediction to list\n",
    "    \n",
    "# get mean of predictions and evaluate on test set    \n",
    "mean_predictions = (np.array(predictions)).mean(axis=0) \n",
    "MICE_imp_score = mean_absolute_error(y_test, mean_predictions)\n",
    "print(f'The MAE of the model using multiple imputation is {MICE_imp_score :.2f}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}